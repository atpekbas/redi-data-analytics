{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Food for thought\n",
    "\n",
    "If you think about, Time Series are all around you. For example, stock market, food prices, how much water you use at home daily or how frequent you opening your Facebook app. Time series, basically put all the information in relation of time.\n",
    "\n",
    "What you have seen in a lecture, that is the gist of it. We could go deeper in mathematic and probability, but the main key to identify how a certian phenonmena behave in time:\n",
    "* Is the electricity price increasing (upward trend)\n",
    "* Are we using more water at the weekend (weekly seasonality)\n",
    "* Sudden increase in online purchase during 2020 (irregularity)\n",
    "* House purchase over the years. (Cyclicity)\n",
    "\n",
    "When you recieve a time series, you want to answer some question, by just looking at the data:\n",
    "* Can I see some trending or seasonal behavior with my data\n",
    "* Can make an educated guess about the next probable value\n",
    "* Does it look like a random noise ? it is? then lets do some crunching, is it really a noise."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing libraries\n",
    "\n",
    "As you may already know, we need some libaries to use some predifined function, so we do not need to reinvent the wheel. So lets use other people work. (That is called open-source)\n",
    "\n",
    "We are going to use in the first part of the exercise the __datetime__ base library and __pandas__ data wrangling library. It is a good practise to use __as__ for shortening the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading the Dataset\n",
    "* Load data with the __pd.read_csv__ function by specifying the location of the dataset, which is in your __data__ folder.\n",
    "* After loading the data, it is good to see how the data was loaded. Sometime, we can spot some errors in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the monthlyMilkProduction.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Analyzing the data\n",
    "\n",
    "After you are confirmed that the data was loaded correctly, you always want to start with getting to know the dataset. Without knowing what our dataset contains, we don't know what it is that we are working with. This step sets up the possibilty for you to identify missing or incorrect data. It is an important prerequisite for data cleaning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cleaning the Data\n",
    "After identifying the general characteristics of our dataset, we can clean it up. Below, you can see a couple of queries that you can perform to make your data cleaner. Clean data is the fundation of a good data analysis! Cleaning the dataset can include, but is not limited to:\n",
    "\n",
    "* Dealing with missing values\n",
    "    * Instead of removing the missing values, let's interpolate (create a new values, based their neighbouring values.)  \n",
    "* Formating date columns\n",
    "    * Let's create a separate date column, plus a separate year and month\n",
    "* Adding calculated columns\n",
    "    * And let's create and extra column based on how many data we have in a year.\n",
    "\n",
    "After we are satisfied with the cleanliness of our dataset, we should always save it under a new name. This allows you to look back at the original dataset easily and have a clear separation between the cleaned and raw dataset. You do not have to run the cleaning code again, and can simply work with the already cleaned dataset next time you open up the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we don't like # and capital letter in the column name, so we will rename it\n",
    "\n",
    "#count missing values by columns\n",
    "\n",
    "\n",
    "#We can see we have 3 in the milk_production column\n",
    "# Let's interpolate the missing values, based on their previous and next values\n",
    "\n",
    "\n",
    "#Did we succeed?\n",
    "\n",
    "\n",
    "#Create the date, year, month  and drop the Month column\n",
    "\n",
    "\n",
    "\n",
    "#Set the column order\n",
    "\n",
    "\n",
    "#Check the data types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets set the year and month as the index\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Exploratory Data Analysis (EDA)\n",
    "\n",
    "* Data Exploration is one of the most crucial part of Time Series analysis. Here we can understand, our dataset, making pleliminary assumptions (hypothesis).\n",
    "* First we can make some descriptive statistic on the dataset, so we know what range our data moving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the descriptive statistics of the data\n",
    "\n",
    "#Which year has the highest milk_production?\n",
    "\n",
    "#Which year has the lowest milk_production?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the the month with the highest yield of milk_production each year\n",
    "\n",
    "\n",
    "#Without plotting we can see that the month with the highest yield of milk production is the same every year (May)\n",
    "#That is indicating some seasonality in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let see how the yearly mean of the milk_production is changing over time\n",
    "\n",
    "\n",
    "#We can see that the mean milk production is increasing over time\n",
    "#That is indicating some trend in the data\n",
    "\n",
    "#lets add date back to the sub dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let see how the yearly standard deviation of the milk production is changing over time\n",
    "\n",
    "\n",
    "#We can see that the standard deviation of milk_production is not increasing over time.\n",
    "#That is indicating that this time series has some additive component\n",
    "\n",
    "#lets add date back to the sub dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Visualisation\n",
    "\n",
    "What we did, by the nummerical/programmatic checking, we can do it sometime just looking at the data. Why sometimes? Because data come every shape and form, and the world random enough to present itself with tricky information, which is not apperent to see for the first glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets add the calculated mean and variance to the plot\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 ext. Exploratory Data Analysis (EDA) EXTRA\n",
    "\n",
    "Lets use some addintional libraries which out of the box helps to decompose our dataset, to see some trend, seasonality and irregularites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decompose the time series into trend, seasonality and residuals\n",
    "\n",
    "#Lets drop the year and month index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets do some decomposition\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are dealing with Time Series, we are doing so to be able to use them to predict the future. We want to see whether the past data can indicate a future behavior and possibly we can take an action, or utilize the predictions.\n",
    "\n",
    "There are miriad of models and algorithms have been developed over the years in order to predict future. But most of them rely on one specific feature of the time series, that is it stationary or not.\n",
    "\n",
    "So let's see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the previous task we have seen that the time series is non-stationary, by looking at the mean and variance\n",
    "#and also we have seen that there is some seasonality in the data by looking at the plot\n",
    "#Lets check if the time series is stationary by using the Augmented Dickey-Fuller test\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Lets create a function to check if the time series is stationary\n",
    "def check_stationarity(timeseries):\n",
    "\n",
    "    #Determing rolling statistics\n",
    "    #Rolling statistics is used to check if the mean and variance is constant over time\n",
    "    rolmean = timeseries.rolling(window=12).mean()\n",
    "    rolstd = timeseries.rolling(window=12).std()\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(timeseries, color=\"blue\", label=\"Original\")\n",
    "    plt.plot(rolmean, color=\"red\", label=\"Rolling Mean\")\n",
    "    plt.plot(rolstd, color=\"black\", label=\"Rolling Std\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Rolling Mean & Standard Deviation\")\n",
    "    plt.show(block=False)\n",
    "\n",
    "    #Perform Dickey-Fuller test:\n",
    "    print(\"Results of Dickey-Fuller Test:\")\n",
    "    dftest = adfuller(timeseries, autolag=\"AIC\")\n",
    "    dfoutput = pd.Series(\n",
    "        dftest[:4],\n",
    "        index=[\n",
    "            \"Test Statistic\",\n",
    "            \"p-value\",\n",
    "            \"#Lags Used\",\n",
    "            \"Number of Observations Used\",\n",
    "        ],\n",
    "    )\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput[f\"Critical Value ({key})\"] = value\n",
    "    print(dfoutput)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmented Dickey Fuller test or (ADF Test) is the most commonly used test to detect stationarity. Here, we assume (inn statistics we say the null-hypothesis) the time series is non-stationary. Then, we collect evidence to support or reject our assumptions(null hypothesis). So, if we find that the __p-value__ in ADF test is less than the significance level (__0.05__), we can reject our assumptions and we can declare the timeseries is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check if the time series is stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets do some differencing , shall we?\n",
    " \n",
    "#Let's plot the differenced time series with the original time series\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can do the same by using the output from the seasonal_decompose function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets remove the seasonality\n",
    "\n",
    "\n",
    "#Lets plot the deseasonalized time series with the original time series\n",
    "\n",
    "\n",
    "#Lets check if the time series is stationary by using the Augmented Dickey-Fuller test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets remove the trend and seasonality\n",
    "\n",
    "\n",
    "#Lets plot the detrended and deseasonalized time series with the original time series\n",
    "\n",
    "\n",
    "#Lets check if the time series is stationary by using the Augmented Dickey-Fuller test\n",
    "\n",
    "#Thats it for this task, in the next task we will look at how to forecast time series data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  7. Interpretation of the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
